{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWn7ZMaVZR8j",
        "outputId": "e9101e3e-8a8a-4761-94c2-ff0b9eabd1b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ],
      "source": [
        "pip install beautifulsoup4 lxml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup as BS\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.5938.92 Safari/537.36',\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
        "    'Accept-Language': 'en-US,en;q=0.5',\n",
        "    'Referer': 'https://www.google.com/',  # Adding a referer can help in bypassing restrictions\n",
        "    'Connection': 'keep-alive'\n",
        "}\n",
        "def getReviews(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 403:\n",
        "        print(\"Access forbidden (403)\")\n",
        "        return None\n",
        "    elif response.status_code != 200:\n",
        "        print(f\"Failed to fetch page, status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "    html_code = response.text\n",
        "    soup = BS(html_code, 'html.parser')\n",
        "\n",
        "    # Extract the total number of reviews\n",
        "    totalReviews = soup.find('span', class_='biGQs _P pZUbB KxBGd').text[:-8].replace(',', '')\n",
        "    hotelName = soup.find('h1', id='HEADING').text\n",
        "\n",
        "    # Extract reviews data\n",
        "    reviewHeads = [i.text for i in soup.find_all('span', class_='JbGkU Cj')]\n",
        "    reviewContent = [i.text for i in soup.find_all('span', class_='orRIx Ci _a C')]\n",
        "    reviewerNdate = [i.text for i in soup.find_all('div', class_='tVWyV _Z o S4 H3 Ci') if 'wrote a review' in i.text]\n",
        "\n",
        "    reviewerandDate = []\n",
        "    for i in reviewerNdate:\n",
        "        reviewerandDate.append(i.split('wrote a review '))\n",
        "    finalDate = []\n",
        "    finalreviewer = []\n",
        "    for i in reviewerandDate:\n",
        "        finalDate.append(i[1])\n",
        "        finalreviewer.append(i[0])\n",
        "\n",
        "    # Extract ratings and hotel location\n",
        "    ratings = [i.find('title').text[0:3] for i in soup.find_all('div', class_='kmMXA _T Gi')]\n",
        "    tempHotelLocation = [i.text for i in soup.find_all('span', class_='biGQs _P pZUbB KxBGd')]\n",
        "    hotelLocation = tempHotelLocation[1]\n",
        "\n",
        "    # Create DataFrame for the reviews\n",
        "    df = pd.DataFrame({\n",
        "        'Hotel Name': hotelName,\n",
        "        'Hotel Location': hotelLocation,\n",
        "        'Reviewer': finalreviewer,\n",
        "        'Rating': ratings,\n",
        "        'Date of Review': finalDate,\n",
        "        'Review': reviewHeads,\n",
        "        'Complete Review': reviewContent\n",
        "    })\n",
        "\n",
        "    return [df, totalReviews, hotelName, hotelLocation]\n",
        "\n",
        "# Function to get additional reviews from paginated URLs\n",
        "def getPureReviews(url, hotelName, hotelLocation):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 403:\n",
        "        print(f\"Access forbidden (403) at {url}\")\n",
        "        return None\n",
        "    elif response.status_code != 200:\n",
        "        print(f\"Failed to fetch page, status code: {response.status_code} at {url}\")\n",
        "        return None\n",
        "\n",
        "    html_code = response.text\n",
        "    soup = BS(html_code, 'html.parser')\n",
        "\n",
        "    # Extract reviews data\n",
        "    reviewHeads = [i.text for i in soup.find_all('span', class_='JbGkU Cj')]\n",
        "    reviewContent = [i.text for i in soup.find_all('span', class_='orRIx Ci _a C')]\n",
        "    reviewerNdate = [i.text for i in soup.find_all('div', class_='tVWyV _Z o S4 H3 Ci') if 'wrote a review' in i.text]\n",
        "\n",
        "    reviewerandDate = []\n",
        "    for i in reviewerNdate:\n",
        "        reviewerandDate.append(i.split('wrote a review '))\n",
        "    finalDate = []\n",
        "    finalreviewer = []\n",
        "    for i in reviewerandDate:\n",
        "        finalDate.append(i[1])\n",
        "        finalreviewer.append(i[0])\n",
        "\n",
        "    ratings = [i.find('title').text[0:3] for i in soup.find_all('div', class_='kmMXA _T Gi')]\n",
        "\n",
        "    # Create DataFrame for the additional reviews\n",
        "    df = pd.DataFrame({\n",
        "        'Hotel Name': hotelName,\n",
        "        'Hotel Location': hotelLocation,\n",
        "        'Reviewer': finalreviewer,\n",
        "        'Rating': ratings,\n",
        "        'Date of Review': finalDate,\n",
        "        'Review': reviewHeads,\n",
        "        'Complete Review': reviewContent\n",
        "    })\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "kgO2HjcfZXQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main part of the script\n",
        "url = \"https://www.tripadvisor.com/Hotel_Review-g580312-d573632-Reviews-Hotel_Astoria_Playa_Only_Adults-Port_d_Alcudia_Alcudia_Majorca_Balearic_Islands.html\"\n",
        "split_url = url.split(\"Reviews-\")\n",
        "first_part = split_url[0] + \"Reviews-or\"\n",
        "second_part = \"-\" + split_url[1]\n",
        "\n",
        "# Get the first set of reviews and hotel data\n",
        "result = getReviews(url)\n",
        "if result is None:\n",
        "    print(\"Failed to retrieve initial data.\")\n",
        "else:\n",
        "    df, totalReviews, hotelName, hotelLocation = result\n",
        "\n",
        "    # Generate URLs for paginated reviews\n",
        "    urls = [f\"{first_part}{i}{second_part}\" for i in range(10, int(totalReviews)-10, 10)]\n",
        "\n",
        "    # Fetch and concatenate reviews from additional pages\n",
        "    for i in urls:\n",
        "        newdf = getPureReviews(i, hotelName, hotelLocation)\n",
        "        if newdf is not None:\n",
        "            df = pd.concat([df, newdf], ignore_index=True)\n",
        "\n",
        "    # Save the reviews to an Excel file\n",
        "    df.to_excel('hotel_reviews9.xlsx', index=False, engine='openpyxl')\n"
      ],
      "metadata": {
        "id": "9l-6h3CDZvS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rdNrYqFVZ1Qy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}